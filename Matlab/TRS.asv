    A1 = enbaht.A1();
    b1 = enbaht.b1();
    A2 = enbaht.A2();
    b2 = enbaht.b2();
    n=20;
    A_list = {A1, A2};
    b_list = {b1, b2};
    maxVal =max(abs(A(:)));
    A= A /51 ;
    % g=@(x) sum(exp(A*x));
    g=@(x) sum((x - 1).^2)
    f=@(x) 0.5  *sum((x - 2).^2)+ g(x) ;
    
    % Define F(x) = f(x) - g(x)
    F = @(x) f(x) - g(x);
    
    %% Stage 1: Compute initial points using CVX
    
    % (a) Compute a feasible point x_g by minimizing g(x)
    cvx_begin quiet
        variable x(n)
        minimize( g(x) )
        subject to
            for i = 1:length(A_list)
                Ai = A_list{i};
                bi = b_list{i};
                Ai * x <= bi;
            end
    cvx_end
     x_g = x;
    % 
    % Compute gradient of g at x_g.
    grad_g = Functions.compute_gradient(g, x_g);
    
    % (b) Compute x_hat by minimizing f(x) - dot(grad_g,x)
    cvx_begin quiet
        variable x_hat(n)
        minimize( f(x) - dot(grad_g, x) )
        subject to
            for i = 1:length(A_list)
                Ai = A_list{i};
                bi = b_list{i};
                Ai * x <= bi;
            end
    cvx_end
    x_hat = x;
    
    % (c) Compute the analytic center x_ac
    cvx_begin quiet
        variables x(n) t(length(A_list))
        minimize( -sum(log(-t+1e-5)) )
        subject to
            for i = 1:length(A_list)
                A_list{i} * x - b_list{i} <= t(i)
            end
    cvx_end
    x_ac = x;
    
    % Compute Hessian and gradient info at x_hat
    H_f   = Functions.hessian_fd(f, x_hat);
    H_phi = Functions.barrier_hessian_symmetric(x_ac, A_list, b_list);
    grad_f = Functions.compute_gradient(f, x_hat);
    
    % (d) Compute x_mid via the hidden convexity method.
    [x_mid, val_hidden] = Functions.hidden_convex(H_f, x_hat, grad_f, H_phi, x_ac);
    q=@(x) f(x_hat)-dot(grad_g,x_hat)+dot(x-x_hat,grad_f-grad_g)+0.5*(x-x_hat)'*H_f*(x-x_hat);
    % (e) Additional candidate: maximize (x_mid - x_hat)'*x subject to constraints.

    cvx_begin quiet
        variable x(n)
        maximize( (x_mid-x_hat)' * x )
        subject to
            for i = 1:length(A_list)
                Ai = A_list{i};
                bi = b_list{i};
                Ai * x <= bi;
            end
    cvx_end
    x_new_1 = x;

   

    %% Candidate Processing (Stages 5 and 6)
    
    tol = 1e-3;       % convergence tolerance
    maxIter = 50;     % maximum iterations
    
    % % Start with two candidate initials: x_new and x_mid.
    candidate_initials = { x_new_1, x_mid,x_new_2 };
%     % Number of additional random candidates you want to add
 num_rand = 3;  

 % Preallocate cell array for random candidates
rand_candidates = cell(1, num_rand);

% Define a box in which to generate random points.
% Adjust the range (here, [-10,10]) as appropriate for your problem.
range_min = -10;
range_max = 10;

for i = 1:num_rand
    % Generate a random point x_rand uniformly in [range_min, range_max]^n
    x_rand = range_min + (range_max - range_min) * rand(n, 1);

    % Project x_rand onto the feasible set defined by your constraints.
    % This is done by minimizing the Euclidean distance between x_rand and x_proj.
    cvx_begin quiet
        variable x_proj(n)
        minimize( norm(x_proj - x_rand, 2) )
        subject to
            for j = 1:length(A_list)
                A_list{j} * x_proj <= b_list{j};
            end
    cvx_end

    % Save the projected candidate.
    rand_candidates{i} = x_proj;
end

% Combine these with your existing candidates.
candidate_initials = [candidate_initials, rand_candidates];

    
    % Initialize best candidate using x_new
    best_obj = F(x_new);
    best_x_final = x_new;
    best_candidate = 0;
    fprintf('Direct x_new objective (F) = %f\n', best_obj);
    
    for idx = 1:length(candidate_initials)
        fprintf('\nProcessing candidate %d...\n', idx);
        candidate = candidate_initials{idx};
        
        %% Stage 5: Gradient Ascent Initialization
        x_current = candidate;
        for i = 1:maxIter
            grad_f_current = Functions.compute_gradient(f, x_current);
            
            cvx_begin quiet
                variable x(n)
                maximize( (grad_f_current - grad_g)' * x )
                subject to
                    for ii = 1:length(A_list)
                        A_list{ii} * x <= b_list{ii};
                    end
            cvx_end
            
            if norm(x - x_current) <= tol
                break;
            end
            x_current = x;
        end
        x_start = x_current;
        stage1_obj = F(x_start);
        fprintf('Candidate %d: Stage 1 objective (F) = %f\n', idx, stage1_obj);
        
        if stage1_obj > best_obj
            best_obj = stage1_obj;
            best_x_final = x_start;
            best_candidate = -idx;  % Negative indicates Stage 1 solution
        end
        
        %% Stage 6: DC Programming Iterations
        % Initialize with linearization at x_start:
        cvx_begin quiet
            variable x(n)
            maximize( Functions.compute_gradient(f, x_start)' * x - g(x) )
            subject to
                for ii = 1:length(A_list)
                    A_list{ii} * x <= b_list{ii};
                end
        cvx_end
        x_current = x;
        
        % Iteratively update:
        for k = 1:maxIter
            cvx_begin quiet
                variable x(n)
                maximize( Functions.compute_gradient(f, x_current)' * x - g(x) )
                subject to
                    for ii = 1:length(A_list)
                        A_list{ii} * x <= b_list{ii};
                    end
            cvx_end
            
            if norm(x - x_current) <= tol
                break;
            end
            x_current = x;
        end
        x_final = x_current;
        stage2_obj = F(x_final);
        fprintf('Candidate %d: Stage 2 objective (F) = %f\n', idx, stage2_obj);
        
        if stage2_obj > best_obj
            best_obj = stage2_obj;
            best_x_final = x_final;
            best_candidate = idx;
        end
    end
    
    best_F = best_obj;
    fprintf('\nBest objective value (F) = %f\n', best_F);
    disp(n)
    Functions.checkFeasibility(best_x_final,A_list,b_list);
    toll=10e-7;
    extreme = Functions.isExtreme(best_x_final, A_list, b_list, toll);
    elapsedTime = toc;
