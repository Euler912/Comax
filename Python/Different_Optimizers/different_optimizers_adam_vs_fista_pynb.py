# -*- coding: utf-8 -*-
"""different_optimizers.ADAM_VS_FISTA.pynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1F0r-4YcSBvf_afkUd3Fvz5tpmjtdEii_
"""

import torch
import numpy as np
from numpy import linalg as LA
import matplotlib.pyplot as plt
from itertools import product

n=2

lambda_reg=0.2

A=np.array([[1 ,2 ],[4, 2]])
b=np.array([1,1])

x=np.random.randn(2)
y=x.copy()
t=1

def f(x):
  return 0.5 * np.linalg.norm(A @ x - b, ord=2)**2 + lambda_reg * np.linalg.norm(x, ord=1)

L=np.linalg.norm(A, ord=2)**2
def soft_threshold(x):
    return np.sign(x) * np.maximum(np.abs(x) - lambda_reg / L, 0)


for i in range(1000):
  x_new=soft_threshold(y-(1/L) * A.T @ (A @ y - b))
  t_new=(1+np.sqrt(1+4*t**2))/2
  y_new=x_new+(t-1)/(t_new)*(x_new-x)
  x=x_new
  y=y_new
  t=t_new
print(f(x))

"""#ADAM which requires $f \in C^{1}$


"""

import torch
import torch.optim as optim

# Check if CUDA is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")


# Define h(x)
def h(x):
    return torch.sum(torch.exp(x))+torch.sum(x**2)

# Initialize x
n =101  # Number of variables
x = torch.tensor([1/22.0] * n, device=device, requires_grad=True)

# Define the optimizer
optimizer = optim.Adam([x], lr=0.6)

# Optimization loop
num_iterations = 101
for i in range(num_iterations):
    optimizer.zero_grad()
    loss = h(x)
    loss.backward()
    optimizer.step()

    # if i % 30== 0:
    #     print(f"Iteration {i}: x = {x.tolist()}, h(x) = {loss.item():.4f}")
print(f"Final result: x = {x.tolist()}")
print(f"h(x) = {h(x).item():.4f}")

